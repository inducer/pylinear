\chapter{Matrix-Free Methods}
\label{cha:matrixfree}

\begin{quote}
   This chapter introduces the notion of an \class{Operator}, which
   is PyLinear's way of expressing matrix-free methods.
\end{quote}

\section{The \class{Operator} concept}

\begin{classdesc*}{Operator}
  An \class{Operator}\index{Operator} is a (typically linear) mapping
  of one vector to another. A matrix is a particularly prominent
  example of this, but \class{Operator}s are mainly used to represent
  vector-to-vector mappings for which no matrix is available (or too
  expensive to compute explicitly).
  \footnote{
    Note, however, that for technical reasons \class{Matrix} classes are
    not automatically \class{Operator} instances--they need to be
    explicitly made into these, as we will see soon.
  }
\end{classdesc*}

Given this single purpose, an \class{Operator} has a pretty 
simple interface:
\begin{memberdesc}[Operator]{shape}
  Returns a tuple \code{(h,w)}, which, in analogy to a \class{Matrix},
  specifies the sizes of the vectors received and returned by the
  \class{Operator}.
\end{memberdesc}
\begin{methoddesc}[Operator]{typecode}{}
  Returns the typecode (see \ref{sec:types-and-flavors}) of the
  \class{Vector}s that this \class{Operator} operates on. This is also
  the typecode of the \class{Vector}s returned by the operations of this
  \class{Operator}. For technical reasons, the two always match.
\end{methoddesc}
\begin{methoddesc}[Operator]{apply}{before, after}
  This method operates on the \class{Vector} \code{before} and returns
  the result of the operation in \code{after}. \code{after}
  needs to be a properly-sized \class{Vector}. Its initial values
  typically do not matter, but may be used as starting guesses,
  for example by iterative solvers. Initializing after to
  all zeroes is always acceptable.
\end{methoddesc}

\section{\class{Operator}s Form an Algebra}

On top of this simple interface of an \class{Operator}, PyLinear 
provides a layer of convenience functions that facilitate the
creation of derived instances, among other things.

\opindex{()}For an \class{Operator} \code{A}, saying \code{A(x)}
with a properly sized and typed \class{Vector} \code{x} will
return the result of applying \code{A} to \code{x}, via the

\opindex{+}For two \class{Operator}s \code{A} and \code{B}, you may write
\code{A+B} to obtain an \class{Operator} mapping that will
perform the operation \code{A(x)+B(x)}. The operator \code{-} works
in an analogous fashion.

\opindex{*}For two \class{Operator}s \code{A} and \code{B}, you may
write \code{A*B} to obtain an \class{Operator} mapping that will
perform the composed operation \code{A(B(x))}. You may also say
\code{a*B} or {B*a} with an \class{Operator} \code{B} and a scalar
\code{a}, and will obtain an \class{Operator} that performs
\code{a*B(x)}. A unary minus \code{-A} returns the negated
operator.

\section{Types of \class{Operator}s}

Matrix-generated are the most obvious kind of \class{Operator}, but 
there are more--and they do not necessarily correspond to a stored
matrix representation.

All the ways to generate \class{Operator} instances are contained 
in the \module{pylinear.operation}. Let's import it:
\begin{verbatim}
  >>> from pylinear.array import *
  >>> import pylinear.operation as op
\end{verbatim}

Each type of operator comes with a constructor class. For example,
matrix operators are constructed by calling the method \method{make}
on the object called \var{MatrixOperator} in the \module{operation} 
module. Consider this example:
\begin{verbatim}
  >>> a = array([[1,2],[3,4]])
  >>> a_op = op.MatrixOperator.make(a)
  >>> v = array([5,6])
  >>> a*v
  >>> a_op(v)
  >>> a_plus_a_op = a_op+a_op
  >>> a_plus_a_op(v)
  >>> four_a_op = 2*a_op + a_plus_a_op
  >>> four_a_op(v)
\end{verbatim}

\begin{classdesc*}{MatrixOperator}
  A \class{MatrixOperator} makes a matrix into an \class{Operator}.
\end{classdesc*}
\begin{methoddesc}{make}{matrix}
  This static method takes a matrix argument and returns a matrix
  operator of the corresponding type.

  It does not make a copy of the matrix, instead, it just keeps
  a reference to the given matrix around.
\end{methoddesc}

Now that you have seen one constructor class, you have basically
seen them all, as they are basically structured in the same way.
What is going to vary from here on down is
\begin{itemize}
\item the name of the constructor class and
\item the arguments of the \method{make} call.
\end{itemize}
Note however that for technical reasons the instance returned by
\method{make} is not a \class{MatrixOperator}:
\begin{verbatim}
  >>> isinstance(a_op, op.MatrixOperator)
\end{verbatim}

So, let's dive right in. The next best thing past applying a linear
operator directly is applying its inverse. Here are some operators to
achieve that. Of course you could always compute the inverse of the
matrix that generates the operator. There are better ways,
however. The simplest one is the \class{LUInverseOperator}:
\begin{classdesc*}{LUInverseOperator}
  A \class{LUInverseOperator} operates on vectors as the inverse of
  the dense matrix it is constructed for, by computing a LU 
  decomposition.
\end{classdesc*}
\begin{methoddesc}{make}{matrix}
  Returns an \class{Operator} whose effect on a vector is $A^{-1}$,
  if $A$ is the given \var{matrix}.

  This is a static method.
\end{methoddesc}
However, for sparse matrices, computing the plain LU decomposition is
rarely feasible. More finesse is required to maintain the sparseness,
and thus the tractability, of the operation. That kind of finesse is
provided by UMFPACK, which is also wrapped in an \class{Operator}
interface in PyLinear.
\begin{classdesc*}{UMFPACKOperator}
  A \class{UMFPACKOperator} operates on vectors as the inverse of a
  \class{SparseExecuteMatrix} given to it.

  Unlike the \class{CGOperator} and the \class{BiCGSTABOperator},
  it does not perform an iterative, but rather a direct method.
  Upon construction, it computes a sparse LU-like decomposition 
  to make actual solving an efficient process.
\end{classdesc*}
\begin{methoddesc}{make}{matrix}
  Returns an \class{Operator} whose effect on a vector is $A^{-1}$,
  if $A$ is the given \var{matrix}, which has to be a 
  \class{SparseExecuteMatrix} instance.

  This is a static method.
\end{methoddesc}
Direct methods like the ones above are important tools, but for some
types of matrices, one can do even better, by means of iterative methods,
which, as an added benefit, do not require a matrix representation of
the operation they are inverting:
\begin{classdesc*}{CGOperator}
  A \class{CGOperator} inverts an \class{Operator} given to it
  by means of the Hestenes/Stiefel Conjugate Gradient method.

  It requires that the matrix representation of the given 
  \class{Operator} be symmetric (or hermitian for complex matrices) 
  and positive definite.
\end{classdesc*}
\begin{methoddesc}{make}{matrix_op, max_it=None, tolerance=1e-12, precon_op=None}
  Returns a \class{CGOperator} that will iteratively approximate the
  inverse of the \class{Operator} \var{matrix_op}. \var{max_it}
  specifies a bound on the number of iterations taken to reach the
  goal of decreasing the residual $\sqrt{(A*x-b)^2}$ by a factor of
  \var{tolerance} (where, obviously, $A$ is a matrix representation of
  \var{matrix_op}, $b$ is the vector to which the \class{CGOperator}
  is applied, and $x$ is the candidate result.  If the given target
  precision is not reached in the given number of iterations, an
  exception is thrown.

  \var{precon_op}, finally, is an approximation to $A^{-1}$ that is
  applied once each iteration. As a preconditioner, it should be
  computationally inexpensive--e.g., if an application of $A$ takes $O(n)$
  computations, so should the preconditioner.

  Notice that neither \var{matrix_op} nor \var{precon_op} are needed
  in matrix form--they are only passed in as operators.

  This is a static method.
\end{methoddesc}
\begin{classdesc*}{BiCGSTABOperator}
  A \class{BiCGSTABOperator} inverts an \class{Operator} given to it
  by means of the BiCGSTAB method.

  It relieves the symmetry requirement of the CG method, but may break
  down for some matrices.
\end{classdesc*}
\begin{methoddesc}{make}{matrix_op, max_it=None, tolerance=1e-12, precon_op=None}
  Returns a \class{BiCGSTABOperator} that will iteratively approximate the
  inverse of \var{matrix_op}. For the parameters, see \code{CGOperator.make}.

  Notice that neither \var{matrix_op} nor \var{precon_op} are needed
  in matrix form--they are only passed in as operators.

  This is a static method.
\end{methoddesc}
One preconditioner that is usable with the above iterative methods is
available as part of NumPy:
\begin{classdesc*}{SSORPreconditioner}
  A \class{SSORPreconditioner} computes an approximate inverse of the
  given matrix: Let $L$ be the lower-left submatrix not including the
  diagonal, and $D$ the diagonal part. Then, for a given parameter
  $\omega$, the SSOR preconditioner is given by 
  \[
  (2-\omega)(D+\omega L)^{-H}(\omega D)(D+\omega L)^{-1},
  \]
  which can be rather efficiently implemented.

  $\omega$ is usually chosen to be one.

  It requires the matrix to be symmetric (or hermitian in the complex
  case).
\end{classdesc*}
\begin{methoddesc}{make}{matrix, omega=1}
  Returns an \class{Operator} whose effect on a vector is $A^{-1}$,
  if $A$ is the given \var{matrix}, which has to be a 
  \class{SparseExecuteMatrix} instance.

  This is a static method.
\end{methoddesc}

\section{Implementing your own \class{Operator}s}

TO BE WRITTEN. Refer to the source in \file{src/operation.py}, 
class \class{LUInverseOperator} for an example. FIXME

For information on implementing your own operators in C++, please
refer to Chapter \ref{cha:extending}.
